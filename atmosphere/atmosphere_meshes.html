<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>MPAS-Atmosphere mesh downloads</title>


<style>
h3.ind
{
text-indent:50px;
}

p.noind
{
max-width:650px;
}

p.ind
{
max-width:600px;
margin-left:50px;
}

p.ind2
{
max-width:575;
margin-left:75px;
}

p.ind3
{
max-width:550;
margin-left:100px;
}

b.red
{
color:red;
}
</style></head><body>

<h1>MPAS-Atmosphere Meshes</h1>

<p class="noind">
Several resolutions of quasi-uniform and refined meshes are
available for download. Each download provides an SCVT mesh on the unit
sphere, the mesh connectivity (graph.info) file for the mesh, and
partitionings of the mesh (e.g., graph.info.part.32) for various MPI
task counts.
</p>

<h2>Creating limited-area subsets of meshes</h2>

<p class="noind">
The MPAS-Atmosphere v7.0 release includes the capability to perform regional simulations.
The regional meshes used in these simulations are formed as subsets of existing meshes.
Currently, the <a href="https://github.com/MiCurry/MPAS-Limited-Area" target="_blank">MPAS-Limited-Area</a>
Python tool is the supported method for generating regional meshes as subsets of any of the meshes
available on this download page. Please refer to the documentation provided with MPAS-Limited-Area
for details of its use.
</p>

<h2>Quasi-uniform meshes</h2>

<h3 class="ind">480-km mesh (2562 horizontal grid cells)</h3>
<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.2562.tar.gz">Download the 480-km mesh</a> (1.6 MB)
</p>

<h3 class="ind">384-km mesh (4002 horizontal grid cells)</h3>
<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.4002.tar.gz">Download the 384-km mesh</a> (5.4 MB)
</p>

<h3 class="ind">240-km mesh (10242 horizontal grid cells)</h3>
<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.10242.tar.gz">Download the 240-km mesh</a> (6.6 MB)
</p>

<h3 class="ind">120-km mesh (40962 horizontal grid cells)</h3>
<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.40962.tar.gz">Download the 120-km mesh</a> (26.9 MB)
</p>

<h3 class="ind">60-km mesh (163842 horizontal grid cells)</h3>
<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.163842.tar.gz">Download the 60-km mesh</a> (111 MB)
</p>

<h3 class="ind">48-km mesh (256002 horizontal grid cells)</h3>
<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.256002.tar.gz">Download the 48-km mesh</a> (206 MB)
</p>

<h3 class="ind">30-km mesh (655362 horizontal grid cells)</h3>
<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.655362.tar.gz">Download the 30-km mesh</a> (456 MB)
</p>

<h3 class="ind">24-km mesh (1024002 horizontal grid cells)</h3>
<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.1024002.tar.gz">Download the 24-km mesh</a> (766 MB)
</p>

<h3 class="ind">15-km mesh (2621442 horizontal grid cells)</h3>
<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.2621442.tar.gz">Download the 15-km mesh</a> (1742 MB)
</p>

<h3 class="ind">12-km mesh (4096002 horizontal grid cells)</h3>

<p class="ind2">
(Download link below following <em>Important notes</em>).
</p>

<h3 class="ind">10-km mesh (5898242 horizontal grid cells)</h3>

<p class="ind2">
(Download link below following <em>Important notes</em>).
</p>

<h3 class="ind">4-km mesh (36864002 horizontal grid cells)</h3>

<p class="ind2">
(Download link below following <em>Important notes</em>).
</p>

<h3 class="ind">3.75-km mesh (41943042 horizontal grid cells)</h3>

<p class="ind2">
(Download link below following <em>Important notes</em>).
</p>

<h3 class="ind">3-km mesh (65536002 horizontal grid cells)</h3>

<p class="ind2">
(Download link below following <em>Important notes</em>).
</p>

<h3 class="ind">Important notes for dense meshes</h3>

<p class="ind">The 3-d fields that exist in the model at higher resolution can easily exceed the
4 GB limit imposed by the classic netCDF format. When creating atmospheric initial conditions (i.e., the "init.nc" file),
and when writing output streams from the model with 3-d fields, <b>it is necessary to use an "io_type" that supports
large variables, such as "pnetcdf,cdf5" or "netcdf4"</b>. For more information on selecting the "io_type" of a stream,
refer to Chapter 5 in the Users' Guide.</p>

<p class="ind">Additionally, depending on the amount of memory available, it may be necessary to interpolate the static,
geographical fields in parallel. In the v5.0 release, doing this properly <b>requires the use of a special, convex mesh
decomposition file -- a regular METIS partition file will not give correct results!</b> In order to process static fields
in parallel, one must:
</p>

<p class="ind">
(1) comment-out the code between <a href="https://github.com/MPAS-Dev/MPAS-Model/blob/v7.0/src/core_init_atmosphere/mpas_init_atm_cases.F#L217-L222">lines 217 and 222</a> in src/core_init_atmosphere/mpas_init_atm_cases.F that ordinarily prevents the parallel processing of static fields; and
</p>

<p class="ind">
(2) ensure that the "cvt" partition file prefix (e.g., x1.5898242.cvt.part., x1.65536002.cvt.part., etc.) is specified
in the config_block_decomp_file_prefix variable in namelist.init_atmosphere.
</p>

<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.4096002.tar.gz">Download the 12-km mesh</a> (2713 MB)
</p>

<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.5898242.tar.gz">Download the 10-km mesh</a> (4380 MB)
</p>

<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.36864002.tar.gz">Download the 4-km mesh</a> (23720 MB)
</p>

<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.41943042.tar.gz">Download the 3.75-km mesh</a> (27246 MB)
</p>

<p class="ind2">
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x1.65536002.tar.gz">Download the 3-km mesh</a> (44700 MB)
</p>

<h2>Variable-resolution meshes</h2>

<p class="ind">
All variable-resolution meshes are supplied with a single refinement 
region that is centered on 0.0&deg; latitude, 0.0&deg; longitude. This region
of refinement may be relocated to other locations on the sphere using the <i>grid_rotate</i>
utility, whose usage is described in the MPAS-Atmosphere Users' Guide.
The <i>grid_rotate</i> utility may be <a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/grid_rotate.tar.gz">downloaded here</a>.
</p>

<h3 class="ind">92-km &#8211; 25-km mesh</h3> 
<p class="ind2"> 
This mesh contains 163842 horizontal grid cells, with the refinement
region spanning approximately 60 degrees of latitude/longitude.
<br>
<br>
<img src="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x4.163842.grid.png" width="450">
<br> 
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x4.163842.tar.gz">Download the mesh</a> (135 MB) 
</p>

<h3 class="ind">46-km &#8211; 12-km mesh</h3> 
<p class="ind2"> 
This mesh is like the 92-km &#8211; 25-km mesh, but with twice the horizontal resolution and 655362 horizontal grid cells.
<br>
<br>
<img src="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x4.655362.grid.png" width="453">
<br> 
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x4.655362.tar.gz">Download the mesh</a> (592 MB) 
</p>

<h3 class="ind">60-km &#8211; 15-km mesh</h3> 
<p class="ind2"> 
This mesh contains 535554 horizontal grid cells, with the refinement
region spanning approximately 55 degrees of latitude and 110 degrees of longitude.
<br>
<br>
<img src="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x4.535554.grid.png" width="450">
<br> 
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x4.535554.tar.gz">Download the mesh</a> (474 MB) 
</p>

<h3 class="ind">60-km &#8211; 10-km mesh</h3> 
<p class="ind2"> 
This mesh contains 999426 horizontal grid cells, with the refinement
region spanning approximately 80 degrees of latitude/longitude.
<br>
<br>
<img src="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x6.999426.grid.png" width="450">
<br> 
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x6.999426.tar.gz">Download the mesh</a> (913 MB) 
</p>

<h3 class="ind">60-km &#8211; 3-km mesh</h3> 
<p class="ind2"> 
This mesh contains 835586 horizontal grid cells, with the refinement
region spanning approximately 16 degrees of latitude/longitude.
<br>
<br>
<img src="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x20.835586.grid.png" width="450">
<br> 
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x20.835586.tar.gz">Download the mesh</a> (628 MB) 
</p>

<h3 class="ind">60-km &#8211; 15-km mesh (Tropical refinement)</h3>
<p class="ind2">
This mesh contains 1572866 horizontal grid cells, with the refinement
region spanning the tropics.
<br>
<br>
<img src="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x4.1572866.grid.png" width="450">
<br>
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x4.1572866.tar.gz">Download the mesh</a> (1177 MB)
</p>

<h3 class="ind">15-km &#8211; 3-km mesh (Circular refinement)</h3> 
<p class="ind2"> 
This mesh contains 6488066 horizontal grid cells, with the circular refinement
region spanning approximately 60 degrees of latitude/longitude. When using this mesh, <b class="red">there are several
crucial changes that must be made to pre-processing steps and to the model configuration</b>:
<br>
<br>
(1) The 3-d fields that exist in the model at higher resolution can easily exceed the
4 GB limit imposed by the classic netCDF format. When creating atmospheric initial conditions (i.e., the "init.nc" file),
and when writing output streams from the model with 3-d fields, <b>it is necessary to use an "io_type" that supports
large variables, such as "pnetcdf,cdf5" or "netcdf4"</b>. For more information on selecting the "io_type" of a stream,
refer to Chapter 5 in the Users' Guide.
<br>
<br>
(2) Depending on the amount of memory available, it may be necessary to interpolate the static,
geographical fields in parallel. In the v5.0 release, doing this properly <b>requires the use of a special, convex mesh
decomposition file -- a regular METIS partition file will not give correct results!</b> In order to process static fields
in parallel, one must:
<br>
</p>
<p class="ind3">
(1) comment-out the code between <a href="https://github.com/MPAS-Dev/MPAS-Model/blob/v7.0/src/core_init_atmosphere/mpas_init_atm_cases.F#L217-L222">lines 217 and 222</a> in src/core_init_atmosphere/mpas_init_atm_cases.F that ordinarily prevents the parallel processing of static fields; and
</p>
<p class="ind3">
(b) ensure that the "cvt" partition file prefix (e.g., x5.6488066.cvt.part.) is specified in the config_block_decomp_file_prefix
variable in namelist.init_atmosphere.</li>
</p>
<p class="ind3">
* Note also that when 'config_native_gwd_static = true', each MPI task will read roughly 4 GB of global terrain data, so care must
be taken to use enough compute nodes to avoid running out of memory. </li>
</p>

<p class="ind2">(3) When running the model itself with the 15-km &#8211; 3-km mesh, it is necessary to set <b>config_apvm_upwinding = 0.0</b> in the nhyd_model namelist record.
</p>

<p class="ind2">
<br>
<br>
<img src="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x5.6488066.grid.png" width="450">
<br> 
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x5.6488066.tar.gz">Download the mesh</a> (4560 MB) 
</p>

<h3 class="ind">15-km &#8211; 3-km mesh (Elliptical refinement)</h3> 
<p class="ind2"> 
This mesh contains x5.8060930 horizontal grid cells, with the elliptical refinement
region spanning approximately 60 degrees of latitude and 80 degrees of longitude. When using this mesh, <b class="red">there are several
crucial changes that must be made to pre-processing steps and to the model configuration</b>:
<br>
<br>
(1) The 3-d fields that exist in the model at higher resolution can easily exceed the
4 GB limit imposed by the classic netCDF format. When creating atmospheric initial conditions (i.e., the "init.nc" file),
and when writing output streams from the model with 3-d fields, <b>it is necessary to use an "io_type" that supports
large variables, such as "pnetcdf,cdf5" or "netcdf4"</b>. For more information on selecting the "io_type" of a stream,
refer to Chapter 5 in the Users' Guide.
<br>
<br>
(2) Depending on the amount of memory available, it may be necessary to interpolate the static,
geographical fields in parallel. In the v5.0 release, doing this properly <b>requires the use of a special, convex mesh
decomposition file -- a regular METIS partition file will not give correct results!</b> In order to process static fields
in parallel, one must:
<br>
</p>
<p class="ind3">
(1) comment-out the code between <a href="https://github.com/MPAS-Dev/MPAS-Model/blob/v7.0/src/core_init_atmosphere/mpas_init_atm_cases.F#L217-L222">lines 217 and 222</a> in src/core_init_atmosphere/mpas_init_atm_cases.F that ordinarily prevents the parallel processing of static fields; and
</p>
<p class="ind3">
(b) ensure that the "cvt" partition file prefix (e.g., x5.8060930.cvt.part.) is specified in the config_block_decomp_file_prefix
variable in namelist.init_atmosphere.</li>
</p>
<p class="ind3">
* Note also that when 'config_native_gwd_static = true', each MPI task will read roughly 4 GB of global terrain data, so care must
be taken to use enough compute nodes to avoid running out of memory. </li>
</p>

<p class="ind2">(3) When running the model itself with the 15-km &#8211; 3-km mesh, it is necessary to set <b>config_apvm_upwinding = 0.0</b> in the nhyd_model namelist record.
</p>

<p class="ind2">
<br>
<br>
<img src="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x5.8060930.grid.png" width="450">
<br> 
<a href="http://www2.mmm.ucar.edu/projects/mpas/atmosphere_meshes/x5.8060930.tar.gz">Download the mesh</a> (5963 MB) 
</p>


</body></html>
